{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "attractive-place",
   "metadata": {},
   "source": [
    "# TAL - Lab05 word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optional-associate",
   "metadata": {},
   "source": [
    "## 1. Test et évalutation d'un modèle entraîné sur Google News"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precise-priority",
   "metadata": {},
   "source": [
    "Téléchargement du moèdle `word2vec`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "manufactured-recorder",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ducky/.conda/envs/tal/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "from gensim.models.word2vec import Text8Corpus\n",
    "from gensim.test.utils import datapath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adolescent-scheduling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell if you don't have the model saved locally\n",
    "w2v_model = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "least-likelihood",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell if you already have the model saved locally\n",
    "path_to_file = '~/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz'\n",
    "w2v_vectors = KeyedVectors.load_word2vec_format(path_to_file, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistical-protocol",
   "metadata": {},
   "source": [
    "> b. Quelle place mémoire occupe le processus du notebook une fois les vecteurs de mots chargés ?  \n",
    "\n",
    "Le processus utilise **~4GB** de mémoire."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indie-gasoline",
   "metadata": {},
   "source": [
    "> c. Quelle est la dimension de l’espace vectoriel dans lequel les mots sont représentés?  \n",
    "\n",
    "La dimension est de **300**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "genetic-developer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_vectors.vector_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registered-prayer",
   "metadata": {},
   "source": [
    "> d. Quelle est la taille du vocabulaire du modèle?  \n",
    "\n",
    "La taille du vocabulaire est du **3'000'000**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fitting-subscription",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is dog in the vocabulary? True\n",
      "is cat in the vocabulary? True\n",
      "is teacher in the vocabulary? True\n",
      "is engineer in the vocabulary? True\n",
      "is switzerland in the vocabulary? True\n",
      "is t-shirt in the vocabulary? False\n",
      "is xylography in the vocabulary? False\n"
     ]
    }
   ],
   "source": [
    "words = [\"dog\", \"cat\", \"teacher\", \"engineer\", \"switzerland\", \"t-shirt\", \"xylography\"]\n",
    "w2v_vectors.vectors.shape\n",
    "\n",
    "for word in words:\n",
    "    print(\"is {} in the vocabulary? {}\".format(word, w2v_vectors.has_index_for(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tender-freeware",
   "metadata": {},
   "source": [
    "> e. Quelle est la distance entre les mots rabbit et carrot?  \n",
    "\n",
    "La distance entre `rabit` et `carrot` est **0.636**. Cette distance et calculé grace à la **similarité cosinus**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ambient-fever",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6369356513023376"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_vectors.distance(\"rabbit\", \"carrot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hispanic-barrier",
   "metadata": {},
   "source": [
    "> f. Considérez 5-10 paires de mots, certains proches sémantiquement, d’autres non.  Pour chaque paire, calculez la distance entre les deux mots.  Veuillez indiquer si les distances obtenues correspondent à vos intuitions sur la proximité des sens des mots.  \n",
    "\n",
    "Correspond a notre intuitions:  \n",
    "* (\"friend\", \"family\") - Oui  \n",
    "* (\"cat\", \"kitten\") - Oui  \n",
    "* (\"sword\", \"ball\") - Oui/Non (on pensait qu'ils seraient plus loin)  \n",
    "* (\"shirt\", \"jeans\") - Oui  \n",
    "* (\"bed\", \"plane\") - Oui  \n",
    "* (\"old\", \"cold\") - Oui  \n",
    "* (\"tea\", \"eejit\") - Oui  \n",
    "* (\"computer\", \"programmer\") - Oui/Non (on pensait qu'ils seraient plus proche)  \n",
    "* (\"nurse\", \"doctor\") - Oui  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "suitable-honolulu",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance between 'friend' & 'family' is 0.5108\n",
      "distance between 'cat' & 'kitten' is 0.2535\n",
      "distance between 'sword' & 'ball' is 0.7112\n",
      "distance between 'shirt' & 'jeans' is 0.4398\n",
      "distance between 'bed' & 'plane' is 0.8260\n",
      "distance between 'old' & 'cold' is 0.9185\n",
      "distance between 'tea' & 'eejit' is 0.9207\n",
      "distance between 'computer' & 'programmer' is 0.5749\n",
      "distance between 'nurse' & 'doctor' is 0.3680\n"
     ]
    }
   ],
   "source": [
    "word_pairs = [\n",
    "    (\"friend\", \"family\"),\n",
    "    (\"cat\", \"kitten\"),\n",
    "    (\"sword\", \"ball\"),\n",
    "    (\"shirt\", \"jeans\"),\n",
    "    (\"bed\", \"plane\"),\n",
    "    (\"old\", \"cold\"),\n",
    "    (\"tea\", \"eejit\"),\n",
    "    (\"computer\", \"programmer\"),\n",
    "    (\"nurse\", \"doctor\"),\n",
    "]\n",
    "\n",
    "for word_pair in word_pairs:\n",
    "    print(\"distance between '{}' & '{}' is {:.4f}\".format(\n",
    "        word_pair[0], \n",
    "        word_pair[1], \n",
    "        w2v_vectors.distance(word_pair[0], word_pair[1])\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amino-berry",
   "metadata": {},
   "source": [
    "> g. Pouvez-vous trouver des mots de sens opposés mais qui sont proches dans l’espace vectoriel?  \n",
    "\n",
    "`buy` et `sell`\n",
    "\n",
    "> Comment expliquez vous cela, et est-ce que c’est selon vous une qualité ou un défaut du modèle word2vec?  \n",
    "\n",
    "Se sont certes des mots de sens opposés mais ils ont un fort lien entre eux (C-à-d l'un ne peu pas exister sans l'autre).\n",
    "C'est clairement une qualité car cela montre que le modèle prend aussi en compte les liens cachés entre les mots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "finished-fortune",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1691538691520691"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_vectors.distance(\"buy\", \"sell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ethical-spanking",
   "metadata": {},
   "source": [
    "> h. Trouvez un mot (anglais) qui possède plusieurs sens différents, et pour chaque sens trouvez un mot sémantiquement proche. (Par exemple, en français, avocat—procureur et avocat—banane.) Comparez les différentes distances. Quel est le défaut du modèle word2vec que vous observez?\n",
    "\n",
    "C'est que ces pairs de mots on des sens très proche, mais ils sont très eloignés selon le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "great-float",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance between 'bow' & 'arrow' is 0.6049\n",
      "distance between 'bow' & 'tie' is 0.7186\n",
      "distance between 'bow' & 'boat' is 0.7089\n"
     ]
    }
   ],
   "source": [
    "# bow - arrow\n",
    "# bow - tie\n",
    "# bow - boat\n",
    "homograph_pairs = [\n",
    "    (\"bow\", \"arrow\"),\n",
    "    (\"bow\", \"tie\"),\n",
    "    (\"bow\", \"boat\")\n",
    "]\n",
    "\n",
    "for homograph_pair in homograph_pairs:\n",
    "    print(\"distance between '{}' & '{}' is {:.4f}\".format(\n",
    "        homograph_pair[0],\n",
    "        homograph_pair[1],\n",
    "        w2v_vectors.distance(homograph_pair[0], homograph_pair[1])\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-transparency",
   "metadata": {},
   "source": [
    "> i. Calculez le score du modèle word2vec sur les données **WordSimilarity-353** et expliquer en 1-2 phrases comment ce score est calculé.\n",
    "\n",
    "Il calcul 3 score different:\n",
    "1. le coefficiant de Pearson \n",
    "2. la correlation de Spearman\n",
    "3. OOV-ratio: Le ratio de pair qui a des mots inconnus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "minus-intervention",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnews_wordsim = w2v_vectors.evaluate_word_pairs(datapath('wordsim353.tsv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "preliminary-google",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention bloc lent !!!!!\n",
    "gnews_qwords = w2v_vectors.evaluate_word_analogies(datapath('questions-words.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wireless-response",
   "metadata": {},
   "source": [
    "> i. En vous aidant de la documentation,calculez le score dumodèle word2vec sur les données **questions-words.txt**. Expliquez en 1-2 phrases comment ce score est calculé (donc ce qu’il mesure)\n",
    "\n",
    "La fonction renvoie un tupple, la première valeur est le ratio de question où le modèle a répondu correctement, la deuxième valeur est les données utilisé pour le score.\n",
    "\n",
    "Le modèle calcul sa réponse en faisant: Athens - Greece + Bangkok et il regarde si Thailand est le mot le plus proche du résultat. Si oui la réponse est considérée comme correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "natural-antigua",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "--------\n",
      "\n",
      "Wordsim353:\n",
      "((0.6238773466616107, 1.7963237724171284e-39), SpearmanrResult(correlation=0.6589215888009288, pvalue=2.5346056459149263e-45), 0.0)\n",
      "\n",
      "questions-words:\n",
      "0.7401448525607863\n"
     ]
    }
   ],
   "source": [
    "print(\"Results:\")\n",
    "print(\"--------\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Wordsim353:\")\n",
    "print(gnews_wordsim)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Questions Words:\")\n",
    "print(gnews_qwords[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flying-thailand",
   "metadata": {},
   "source": [
    "## 2. Entraîner deux nouveaux modèles word2vec à partir de nouveaux corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funny-invite",
   "metadata": {},
   "source": [
    "> a. En utilisant `gensim.downloader`, récupérez le corpus qui contient les 10⁸ premiers caractères de Wikipédia (en anglais) avec la commande : `corpus = gensim.downloader.load('text8')`. Combien de phrases et de mots (tokens) possède ce corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "important-soundtrack",
   "metadata": {},
   "outputs": [],
   "source": [
    "text8_corpus = api.load('text8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "saving-national",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences 1701\n"
     ]
    }
   ],
   "source": [
    "# Nombre de phrases : 1701\n",
    "print(\"Number of sentences\", api.info('text8')['num_records'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "standing-complexity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words:  17005207\n"
     ]
    }
   ],
   "source": [
    "# Nombre de mots : 17005207\n",
    "print(\"Number of words: \", sum(len(i) for i in text8_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gothic-questionnaire",
   "metadata": {},
   "source": [
    "> b. Entraînez un nouveau modèle word2vec sur ce nouveau corpus. Si nécessaire, procédez progressivement, en commençant par **1%** du corpus, puis **10%**, pour contrôler le temps nécessaire.\n",
    "> - Indiquez la dimension choisie pour le *embedding* de ce nouveau modèle\n",
    "> - Combien de temps prend l’entraînement sur le corpus total?\n",
    "> - Quelle est la taille (en Mo) du modèle word2vec résultant ?\n",
    "\n",
    "Temps = 2 minutes  \n",
    "Taille du modele = 57 Mo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "turned-aspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell if you don't have the model saved locally\n",
    "text8_model = Word2Vec(corpus, vector_size=300)\n",
    "text8_model.save(\"text8.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "frequent-country",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell if you already have the model saved locally\n",
    "text8_model = KeyedVectors.load(\"text8.model\", mmap='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaging-teens",
   "metadata": {},
   "source": [
    "> c. Mesurez la qualité de ce modèle comme dans la partie 1, points i et j. Ce modèle est-il meilleur que celui entraîné sur Google News? Quelle serait la raison de la différence?\n",
    "\n",
    "Ce modèle n'est pas meilleur que celui qui est entraîné sur Google News. La raison de cette différence provient de la taille des datasets. Dans ce cas, le dataset possède 71k tokens tandis que celui de Google news en a 3M."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "reserved-mexican",
   "metadata": {},
   "outputs": [],
   "source": [
    "text8_wv = text8_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "sensitive-motor",
   "metadata": {},
   "outputs": [],
   "source": [
    "text8_wordsim = text8_wv.evaluate_word_pairs(datapath('wordsim353.tsv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "surprising-kernel",
   "metadata": {},
   "outputs": [],
   "source": [
    "text8_qwords = text8_wv.evaluate_word_analogies(datapath('questions-words.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "enormous-painting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "--------\n",
      "\n",
      "Wordsim353:\n",
      "Google News: ((0.6238773466616107, 1.7963237724171284e-39), SpearmanrResult(correlation=0.6589215888009288, pvalue=2.5346056459149263e-45), 0.0)\n",
      "Text8: ((0.6105890642318692, 3.072339335590552e-37), SpearmanrResult(correlation=0.6246376132773954, pvalue=2.2456458997037836e-39), 0.56657223796034)\n",
      "\n",
      "Questions Words:\n",
      "Google News: 0.7393798449612403\n",
      "Text8: 0.26256513699781475\n"
     ]
    }
   ],
   "source": [
    "print(\"Results:\")\n",
    "print(\"--------\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Wordsim353:\")\n",
    "print(\"Google News:\", gnews_wordsim)\n",
    "print(\"Text8:\", text8_wordsim)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Questions Words:\")\n",
    "print(\"Google News:\", gnews_qwords[0])\n",
    "print(\"Text8:\", text8_qwords[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corresponding-campbell",
   "metadata": {},
   "source": [
    "> d. Téléchargez maintenant le corpus quatre fois plus grandc onstitué de la concaténation du corpus `text8` et des dépêches économiques de Reuters (413 Mo) fourni en ligne par l’enseignant et appelé `wikipedia_augmented.dat`. Entraînez un nouveau modèle word2vecsur ce corpus, en précisant la dimension du plongement (embedding). \n",
    "> - Utilisez la classe `Text8Corpus()` pourcharger le corpus et faire la tokenization et la segmentation en phrases\n",
    "> - Combien de temps prend l’entraînement?\n",
    "> - Quelle est la taille (en Mo) du modèle word2vec résultant?\n",
    "\n",
    "Temps = 5 minutes  \n",
    "Taille = 100M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "accessible-cleaners",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell if you don't have the model saved locally\n",
    "wiki_augmented_model = Word2Vec(Text8Corpus('wikipedia_augmented.dat'))\n",
    "wiki_augmented_model.save(\"wiki_augmented.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "loved-georgia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell if you already have the model saved locally\n",
    "wiki_augmented_model = Word2Vec.load(\"wiki_augmented.model\", mmap='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-sampling",
   "metadata": {},
   "source": [
    "> e. Testez ce modèle comme en 1.i et 1.j.  Est-il meilleur que le précédent? Pour quelle raison?\n",
    "\n",
    "Il est meilleur que notre modèle sur text-8. Néanmoins, il est moins bon que celui de Google News. La raison est la même que précédemment. Il y a une grande différence de taille (~120K vs 3M)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "violent-roman",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_augmented_wv = wiki_augmented_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "herbal-bosnia",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_wordsim = wiki_augmented_wv.evaluate_word_pairs(datapath('wordsim353.tsv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "electronic-speech",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_qwords = wiki_augmented_wv.evaluate_word_analogies(datapath('questions-words.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "minute-shield",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "--------\n",
      "\n",
      "Wordsim353:\n",
      "Google News: ((0.6238773466616107, 1.7963237724171284e-39), SpearmanrResult(correlation=0.6589215888009288, pvalue=2.5346056459149263e-45), 0.0)\n",
      "Text8: ((0.6105890642318692, 3.072339335590552e-37), SpearmanrResult(correlation=0.6246376132773954, pvalue=2.2456458997037836e-39), 0.56657223796034)\n",
      "Wikipedia augmented: ((0.5050250395258843, 3.0228907961391866e-24), SpearmanrResult(correlation=0.512229711290635, pvalue=5.258117495412604e-25), 0.0)\n",
      "\n",
      "Questions Words:\n",
      "Google News: 0.7393798449612403\n",
      "Text8: 0.26256513699781475\n",
      "Wikipedia augmented: 0.34995654063450676\n"
     ]
    }
   ],
   "source": [
    "print(\"Results:\")\n",
    "print(\"--------\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Wordsim353:\")\n",
    "print(\"Google News:\", gnews_wordsim)\n",
    "print(\"Text8:\", text8_wordsim)\n",
    "print(\"Wikipedia augmented:\", wiki_wordsim)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Questions Words:\")\n",
    "print(\"Google News:\", gnews_qwords[0])\n",
    "print(\"Text8:\", text8_qwords[0])\n",
    "print(\"Wikipedia augmented:\", wiki_qwords[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checked-kernel",
   "metadata": {},
   "source": [
    "> f. Créez un nouveau fichier de test en augmentant `questions-words.txt` avec environ 20 items de test. Par exemple, à partir de `{(eye, see), (ear, listen), (foot, walk)}` on peut construire 6 items de test. Testez les trois modèles sur ces nouvelles données, puis commentez vos résultats\n",
    "\n",
    "Il n'y a eu aucun changements. Ce qui est normal étant donnée que nous avons seulement ajouté 20 items de test ce qui n'est pas grand chose étant donné qu'il y a déjà ~20'000 qestions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "configured-municipality",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnews_qwords = w2v_vectors.evaluate_word_analogies('questions-words-custom.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "animated-species",
   "metadata": {},
   "outputs": [],
   "source": [
    "text8_qwords = text8_wv.evaluate_word_analogies('questions-words-custom.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "curious-guidance",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_qwords = wiki_augmented_wv.evaluate_word_analogies('questions-words-custom.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "diverse-guatemala",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "--------\n",
      "Google News: 0.7393798449612403\n",
      "Text8: 0.26256513699781475\n",
      "Wikipedia augmented: 0.34995654063450676\n"
     ]
    }
   ],
   "source": [
    "print(\"Results:\")\n",
    "print(\"--------\")\n",
    "print(\"Google News:\", gnews_qwords[0])\n",
    "print(\"Text8:\", text8_qwords[0])\n",
    "print(\"Wikipedia augmented:\", wiki_qwords[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
